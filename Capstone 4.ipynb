{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import sklearn as ensemble\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/bin/python\n",
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday= gutenberg.raw('chesterton-thursday.txt')\n",
    "bryant= gutenberg.raw('bryant-stories.txt')\n",
    "burgess= gutenberg.raw('burgess-busterbrown.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "brown= gutenberg.raw('chesterton-brown.txt')\n",
    "paradise= gutenberg.raw('milton-paradise.txt')\n",
    "whitman= gutenberg.raw('whitman-leaves.txt')\n",
    "sense= gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "poem = re.sub(pattern, \"\", poem)\n",
    "thursday = re.sub(pattern, \"\", thursday)\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "bryant = re.sub(pattern, \"\", bryant)\n",
    "brown = re.sub(pattern, \"\", brown)\n",
    "paradise = re.sub(pattern, \"\", paradise)\n",
    "whitman = re.sub(pattern, \"\", whitman)\n",
    "sense = re.sub(pattern, \"\", sense)\n",
    "burgess = re.sub(pattern, \"\", burgess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'Chapter \\d+', '', alice)\n",
    "bryant = re.sub(r'Chapter \\d+', '', bryant)\n",
    "brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "paradise = re.sub(r'Chapter \\d+', '', paradise)\n",
    "whitman = re.sub(r'Chapter \\d+', '', whitman)\n",
    "sense = re.sub(r'Chapter \\d+', '', sense)\n",
    "burgess = re.sub(r'Chapter \\d+', '', burgess)\n",
    "\n",
    "\n",
    "poem = ' '.join(poem.split())\n",
    "thursday = ' '.join(thursday.split())\n",
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "bryant = ' '.join(bryant.split())\n",
    "brown = ' '.join(brown.split())\n",
    "paradise = ' '.join(paradise.split())\n",
    "whitman= ' '.join(whitman.split())\n",
    "sense = ' '.join(sense.split())\n",
    "burgess = ' '.join(burgess.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping dow\n",
      "Extra whitespace removed:\n",
      " To Edmund Clerihew Bentley A cloud was on the mind of men, and wailing went the weather, Yea, a sick\n",
      "Extra whitespace removed:\n",
      " Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who, for his own amusement, never t\n",
      "Extra whitespace removed:\n",
      " CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on th\n",
      "Extra whitespace removed:\n",
      " TWO LITTLE RIDDLES IN RHYME There's a garden that I ken, Full of little gentlemen; Little caps of bl\n",
      "Extra whitespace removed:\n",
      " I. The Absence of Mr Glass THE consulting-rooms of Dr Orion Hood, the eminent criminologist and spec\n",
      "Extra whitespace removed:\n",
      " Book I Of Man's first disobedience, and the fruit Of that forbidden tree whose mortal taste Brought \n",
      "Extra whitespace removed:\n",
      " Come, said my soul, Such verses for my Body let us write, (for we are one,) That should I after retu\n",
      "Extra whitespace removed:\n",
      " CHAPTER 1 The family of Dashwood had long been settled in Sussex. Their estate was large, and their \n",
      "Extra whitespace removed:\n",
      " I BUSTER BEAR GOES FISHING Buster Bear yawned as he lay on his comfortable bed of leaves and watched\n"
     ]
    }
   ],
   "source": [
    "print('Extra whitespace removed:\\n', poem[0:100])\n",
    "print('Extra whitespace removed:\\n', thursday[0:100])\n",
    "print('Extra whitespace removed:\\n', persuasion[0:100])\n",
    "print('Extra whitespace removed:\\n', alice[0:100])\n",
    "print('Extra whitespace removed:\\n', bryant[0:100])\n",
    "print('Extra whitespace removed:\\n', brown[0:100])\n",
    "print('Extra whitespace removed:\\n', paradise[0:100])\n",
    "print('Extra whitespace removed:\\n', whitman[0:100])\n",
    "print('Extra whitespace removed:\\n', sense[0:100])\n",
    "print('Extra whitespace removed:\\n', burgess[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.max_length\n",
    "\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "alice_doc = nlp(alice)\n",
    "bryant_doc = nlp(bryant)\n",
    "brown_doc = nlp(brown)\n",
    "paradise_doc = nlp(paradise)\n",
    "whitman_doc= nlp(whitman)\n",
    "sense_doc = nlp(sense)\n",
    "burgess_doc = nlp(burgess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "bryantwords = bag_of_words(bryant_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "brownwords = bag_of_words(paradise_doc)\n",
    "whitmanwords= bag_of_words(whitman_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "burgesswords = bag_of_words(burgess_doc)\n",
    "\n",
    "\n",
    "\n",
    "common_words = set(poemwords + thursdaywords + persuasionwords + alicewords + bryantwords  + \n",
    "                   brownwords + brownwords + whitmanwords + sensewords + burgesswords )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "  \n",
    "    return Counter(words)\n",
    "    \n",
    "poem_freq = word_frequencies(poem_doc).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "bryant_freq = word_frequencies(bryant_doc).most_common(10)\n",
    "brown_freq = word_frequencies(brown_doc).most_common(10)\n",
    "brown_freq = word_frequencies(paradise_doc).most_common(10)\n",
    "whitman_freq= word_frequencies(whitman_doc).most_common(10)\n",
    "sense_freq = word_frequencies(sense_doc).most_common(10)\n",
    "burgess_freq = word_frequencies(burgess_doc).most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_freq = word_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc,include_stop=False).most_common(10)\n",
    "alice_freq = word_frequencies(alice_doc,include_stop=False).most_common(10)\n",
    "bryant_freq = word_frequencies(bryant_doc,include_stop=False).most_common(10)\n",
    "brown_freq = word_frequencies(brown_doc,include_stop=False).most_common(10)\n",
    "paradise_freq = word_frequencies(paradise_doc,include_stop=False).most_common(10)\n",
    "whitman_freq= word_frequencies(whitman_doc,include_stop=False).most_common(10)\n",
    "sense_freq = word_frequencies(sense_doc,include_stop=False).most_common(10)\n",
    "burgess_freq = word_frequencies(burgess_doc,include_stop=False).most_common(10)\n",
    "\n",
    "freq_words = set(poem_freq  + thursday_freq + persuasion_freq + alice_freq + bryant_freq  + \n",
    "                   brown_freq + paradise_freq + whitman_freq + sense_freq + burgess_freq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem_common = [pair[0] for pair in poem_freq]\n",
    "thursday_common = [pair[0] for pair in thursday_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "bryant_common = [pair[0] for pair in bryant_freq]\n",
    "brown_common = [pair[0] for pair in brown_freq]\n",
    "paradise_common = [pair[0] for pair in paradise_freq]\n",
    "whitman_common= [pair[0] for pair in whitman_freq]\n",
    "sense_common = [pair[0] for pair in sense_freq]\n",
    "burgess_common = [pair[0] for pair in burgess_freq]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    return Counter(lemmas)\n",
    "\n",
    "\n",
    "poem_lemma_freq = lemma_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_lemma_freq = lemma_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = word_frequencies(persuasion_doc,include_stop=False).most_common(10)\n",
    "alice_lemma_freq = word_frequencies(alice_doc,include_stop=False).most_common(10)\n",
    "bryant_lemma_freq = word_frequencies(bryant_doc,include_stop=False).most_common(10)\n",
    "brown_lemma_freq = word_frequencies(brown_doc,include_stop=False).most_common(10)\n",
    "paradise_lemma_freq = word_frequencies(paradise_doc,include_stop=False).most_common(10)\n",
    "whitman_lemma_freq= word_frequencies(whitman_doc,include_stop=False).most_common(10)\n",
    "sense_lemma_freq = word_frequencies(sense_doc,include_stop=False).most_common(10)\n",
    "burgess_lemma_freq = word_frequencies(burgess_doc,include_stop=False).most_common(10)\n",
    "\n",
    "\n",
    "poem_lemma_common = [pair[0] for pair in poem_lemma_freq]\n",
    "thursday_lemma_common = [pair[0] for pair in thursday_lemma_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "alice_common = [pair[0] for pair in alice_lemma_freq]\n",
    "bryant_common = [pair[0] for pair in bryant_lemma_freq]\n",
    "brown_common = [pair[0] for pair in brown_lemma_freq]\n",
    "paradise_common = [pair[0] for pair in paradise_lemma_freq]\n",
    "whitman_common= [pair[0] for pair in whitman_lemma_freq]\n",
    "sense_common = [pair[0] for pair in sense_lemma_freq]\n",
    "burgess_common = [pair[0] for pair in burgess_lemma_freq]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(poem_doc.sents)\n",
    "print(\"Poem has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp(\"I need a break\")[3].pos_)\n",
    "print(nlp(\"I need to break the glass\")[3].pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = list(thursday_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [entity.text for entity in list(thursday_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "    \n",
    "thursday = text_cleaner(thursday[:int(len(thursday)/10)])\n",
    "poem = text_cleaner(poem[:int(len(poem)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-21b1224cc9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfreq_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# The coordinates of the centers of our blobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1554\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'set'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= common_words\n",
    "y= freq_words\n",
    "\n",
    "sklearn.preprocessing.normalize(X,  axis=1, copy=True, return_norm=False)\n",
    "\n",
    "# The coordinates of the centers of our blobs.\n",
    "centers = [[2, 2], [-2, -2], [2, -2]]\n",
    "\n",
    "# Make 10,000 rows worth of data with two features representing three\n",
    "# clusters, each having a standard deviation of 1.\n",
    "X, y = make_blobs(\n",
    "    n_samples=10000,\n",
    "    centers=centers,\n",
    "    cluster_std=1,\n",
    "    n_features=2,\n",
    "    random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "#Divide into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.9,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
