{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import sklearn as ensemble\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/bin/python\n",
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "\n",
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday= gutenberg.raw('chesterton-thursday.txt')\n",
    "bryant= gutenberg.raw('bryant-stories.txt')\n",
    "burgess= gutenberg.raw('burgess-busterbrown.txt')\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "brown= gutenberg.raw('chesterton-brown.txt')\n",
    "paradise= gutenberg.raw('milton-paradise.txt')\n",
    "whitman= gutenberg.raw('whitman-leaves.txt')\n",
    "sense= gutenberg.raw('austen-sense.txt')\n",
    "\n",
    "\n",
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'Chapter \\d+', '', alice)\n",
    "bryant = re.sub(r'Chapter \\d+', '', bryant)\n",
    "brown = re.sub(r'Chapter \\d+', '', brown)\n",
    "paradise = re.sub(r'Chapter \\d+', '', paradise)\n",
    "whitman = re.sub(r'Chapter \\d+', '', whitman)\n",
    "sense = re.sub(r'Chapter \\d+', '', sense)\n",
    "burgess = re.sub(r'Chapter \\d+', '', burgess)\n",
    "\n",
    "    \n",
    "poem = text_cleaner(poem[:int(len(poem)/30)])\n",
    "thursday = text_cleaner(thursday[:int(len(thursday)/30)])\n",
    "alice = text_cleaner(alice[:int(len(alice)/30)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/30)])\n",
    "bryant = text_cleaner(bryant[:int(len(bryant)/30)])\n",
    "brown = text_cleaner(brown[:int(len(brown)/30)])\n",
    "paradise = text_cleaner(paradise[:int(len(paradise)/30)])\n",
    "whitman = text_cleaner(whitman[:int(len(whitman)/30)])\n",
    "sense = text_cleaner(sense[:int(len(sense)/30)])\n",
    "burgess = text_cleaner(burgess[:int(len(burgess)/30)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.max_length\n",
    "\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)\n",
    "persuasion_doc = nlp(persuasion)\n",
    "alice_doc = nlp(alice)\n",
    "bryant_doc = nlp(bryant)\n",
    "brown_doc = nlp(brown)\n",
    "paradise_doc = nlp(paradise)\n",
    "whitman_doc= nlp(whitman)\n",
    "sense_doc = nlp(sense)\n",
    "burgess_doc = nlp(burgess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(CHAPTER, I.)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0                                      (CHAPTER, I.)  Carroll\n",
       "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...  Carroll\n",
       "2  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "3  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_sents = [[sent, \"Blake\"] for sent in poem_doc.sents]\n",
    "thursday_sents = [[sent, \"Chesterton\"] for sent in thursday_doc.sents]\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "bryant_sents = [[sent, \"Bryant\"] for sent in bryant_doc.sents]\n",
    "brown_sents = [[sent, \"Chesterton-Brown\"] for sent in brown_doc.sents]\n",
    "paradise_sents = [[sent, \"Milton\"] for sent in paradise_doc.sents]\n",
    "whitman_sents = [[sent, \"Whitman\"] for sent in whitman_doc.sents]\n",
    "sense_sents = [[sent, \"Austen-Sense\"] for sent in sense_doc.sents]\n",
    "burgess_sents = [[sent, \"Burgess\"] for sent in burgess_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents + poem_sents + thursday_sents + bryant_sents + brown_sents \n",
    "                        + paradise_sents +  whitman_sents + sense_sents + burgess_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "bryantwords = bag_of_words(bryant_doc)\n",
    "brownwords = bag_of_words(brown_doc)\n",
    "brownwords = bag_of_words(paradise_doc)\n",
    "whitmanwords= bag_of_words(whitman_doc)\n",
    "sensewords = bag_of_words(sense_doc)\n",
    "burgesswords = bag_of_words(burgess_doc)\n",
    "\n",
    "\n",
    "\n",
    "common_words = set(poemwords + thursdaywords + persuasionwords + alicewords + bryantwords  + \n",
    "                   brownwords + brownwords + whitmanwords + sensewords + burgesswords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n",
      "Processing row 450\n",
      "Processing row 500\n",
      "Processing row 550\n",
      "Processing row 600\n",
      "Processing row 650\n",
      "Processing row 700\n",
      "Processing row 750\n",
      "Processing row 800\n",
      "Processing row 850\n",
      "Processing row 900\n",
      "Processing row 950\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expedient</th>\n",
       "      <th>spiritual</th>\n",
       "      <th>anger</th>\n",
       "      <th>with</th>\n",
       "      <th>write</th>\n",
       "      <th>solitary</th>\n",
       "      <th>mansoul</th>\n",
       "      <th>associate</th>\n",
       "      <th>perfume</th>\n",
       "      <th>gr</th>\n",
       "      <th>...</th>\n",
       "      <th>walk</th>\n",
       "      <th>meditate</th>\n",
       "      <th>germ</th>\n",
       "      <th>exercise</th>\n",
       "      <th>disregard</th>\n",
       "      <th>bad</th>\n",
       "      <th>self</th>\n",
       "      <th>grass</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(CHAPTER, I.)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Down, the, Rabbit, -, Hole, Alice, was, begin...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  expedient spiritual anger with write solitary mansoul associate perfume gr  \\\n",
       "0         0         0     0    0     0        0       0         0       0  0   \n",
       "1         0         0     0    0     0        0       0         0       0  0   \n",
       "2         0         0     0    0     0        0       0         0       0  0   \n",
       "3         0         0     0    0     0        0       0         0       0  0   \n",
       "4         0         0     0    0     0        0       0         0       0  0   \n",
       "\n",
       "   ... walk meditate germ exercise disregard bad self grass  \\\n",
       "0  ...    0        0    0        0         0   0    0     0   \n",
       "1  ...    0        0    0        0         0   0    0     0   \n",
       "2  ...    0        0    0        0         0   0    0     0   \n",
       "3  ...    0        0    0        0         0   0    0     0   \n",
       "4  ...    0        0    0        0         0   0    0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                                      (CHAPTER, I.)     Carroll  \n",
       "1  (Down, the, Rabbit, -, Hole, Alice, was, begin...     Carroll  \n",
       "2  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "3  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "4                                      (Oh, dear, !)     Carroll  \n",
       "\n",
       "[5 rows x 3050 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    expedient spiritual anger with write solitary mansoul associate perfume  \\\n",
      "0           0         0     0    0     0        0       0         0       0   \n",
      "1           0         0     0    0     0        0       0         0       0   \n",
      "2           0         0     0    0     0        0       0         0       0   \n",
      "3           0         0     0    0     0        0       0         0       0   \n",
      "4           0         0     0    0     0        0       0         0       0   \n",
      "5           0         0     0    0     0        0       0         0       0   \n",
      "6           0         0     0    0     0        0       0         0       0   \n",
      "7           0         0     0    0     0        0       0         0       0   \n",
      "8           0         0     0    0     0        0       0         0       0   \n",
      "9           0         0     0    0     0        0       0         0       0   \n",
      "10          0         0     0    0     0        0       0         0       0   \n",
      "11          0         0     0    0     0        0       0         0       0   \n",
      "12          0         0     0    0     0        0       0         0       0   \n",
      "13          0         0     0    0     0        0       0         0       0   \n",
      "14          0         0     0    0     0        0       0         0       0   \n",
      "15          0         0     0    0     0        0       0         0       0   \n",
      "16          0         0     0    0     0        0       0         0       0   \n",
      "17          0         0     0    0     0        0       0         0       0   \n",
      "18          0         0     0    0     0        0       0         0       0   \n",
      "19          0         0     0    0     0        0       0         0       0   \n",
      "20          0         0     0    0     0        0       0         0       0   \n",
      "21          0         0     0    0     0        0       0         0       0   \n",
      "22          0         0     0    0     0        0       0         0       0   \n",
      "23          0         0     0    0     0        0       0         0       0   \n",
      "24          0         0     0    0     0        0       0         0       0   \n",
      "25          0         0     0    0     0        0       0         0       0   \n",
      "26          0         0     0    0     0        0       0         0       0   \n",
      "27          0         0     0    0     0        0       0         0       0   \n",
      "28          0         0     0    0     0        0       0         0       0   \n",
      "29          0         0     0    0     0        0       0         0       0   \n",
      "..        ...       ...   ...  ...   ...      ...     ...       ...     ...   \n",
      "929         0         0     0    0     0        0       0         0       0   \n",
      "930         0         0     0    0     0        0       0         0       0   \n",
      "931         0         0     0    0     0        0       0         0       0   \n",
      "932         0         0     0    0     0        0       0         0       0   \n",
      "933         0         0     0    0     0        0       0         0       0   \n",
      "934         0         0     0    0     0        0       0         0       0   \n",
      "935         0         0     0    0     0        0       0         0       0   \n",
      "936         0         0     0    0     0        0       0         0       0   \n",
      "937         0         0     0    0     0        0       0         0       0   \n",
      "938         0         0     0    0     0        0       0         0       0   \n",
      "939         0         0     0    0     0        0       0         0       0   \n",
      "940         0         0     0    0     0        0       0         0       0   \n",
      "941         0         0     0    0     0        0       0         0       0   \n",
      "942         0         0     0    0     0        0       0         0       0   \n",
      "943         0         0     0    0     0        0       0         0       0   \n",
      "944         0         0     0    0     0        0       0         0       0   \n",
      "945         0         0     0    0     0        0       0         0       0   \n",
      "946         0         0     0    0     0        0       0         0       0   \n",
      "947         0         0     0    0     0        0       0         0       0   \n",
      "948         0         0     0    0     0        0       0         0       0   \n",
      "949         0         0     0    0     0        0       0         0       0   \n",
      "950         0         0     0    0     0        0       0         0       0   \n",
      "951         0         0     0    0     0        0       0         0       0   \n",
      "952         0         0     0    0     0        0       0         0       0   \n",
      "953         0         0     0    0     0        0       0         0       0   \n",
      "954         0         0     0    0     0        0       0         0       0   \n",
      "955         0         0     0    0     0        0       0         0       0   \n",
      "956         0         0     0    0     0        0       0         0       0   \n",
      "957         0         0     0    0     0        0       0         0       0   \n",
      "958         0         0     0    0     0        0       0         0       0   \n",
      "\n",
      "    gr  ... unusual provide walk meditate germ exercise disregard bad self  \\\n",
      "0    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "1    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "2    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "3    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "4    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "5    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "6    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "7    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "8    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "9    0  ...       0       0    0        0    0        0         0   0    0   \n",
      "10   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "11   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "12   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "13   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "14   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "15   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "16   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "17   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "18   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "19   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "20   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "21   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "22   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "23   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "24   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "25   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "26   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "27   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "28   0  ...       0       0    1        0    0        0         0   0    0   \n",
      "29   0  ...       0       0    0        0    0        0         0   0    0   \n",
      "..  ..  ...     ...     ...  ...      ...  ...      ...       ...  ..  ...   \n",
      "929  0  ...       0       0    1        0    0        0         0   0    0   \n",
      "930  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "931  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "932  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "933  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "934  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "935  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "936  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "937  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "938  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "939  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "940  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "941  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "942  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "943  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "944  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "945  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "946  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "947  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "948  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "949  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "950  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "951  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "952  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "953  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "954  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "955  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "956  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "957  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "958  0  ...       0       0    0        0    0        0         0   0    0   \n",
      "\n",
      "    grass  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "5       0  \n",
      "6       0  \n",
      "7       0  \n",
      "8       0  \n",
      "9       0  \n",
      "10      0  \n",
      "11      0  \n",
      "12      0  \n",
      "13      0  \n",
      "14      0  \n",
      "15      0  \n",
      "16      0  \n",
      "17      0  \n",
      "18      0  \n",
      "19      0  \n",
      "20      0  \n",
      "21      0  \n",
      "22      0  \n",
      "23      0  \n",
      "24      0  \n",
      "25      0  \n",
      "26      0  \n",
      "27      0  \n",
      "28      0  \n",
      "29      0  \n",
      "..    ...  \n",
      "929     0  \n",
      "930     0  \n",
      "931     0  \n",
      "932     0  \n",
      "933     0  \n",
      "934     0  \n",
      "935     0  \n",
      "936     0  \n",
      "937     0  \n",
      "938     0  \n",
      "939     0  \n",
      "940     0  \n",
      "941     0  \n",
      "942     0  \n",
      "943     0  \n",
      "944     0  \n",
      "945     0  \n",
      "946     0  \n",
      "947     0  \n",
      "948     0  \n",
      "949     0  \n",
      "950     0  \n",
      "951     0  \n",
      "952     0  \n",
      "953     0  \n",
      "954     0  \n",
      "955     0  \n",
      "956     0  \n",
      "957     0  \n",
      "958     0  \n",
      "\n",
      "[959 rows x 3048 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (word_counts.iloc[:,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_counts.iloc[:,:-2]\n",
    "y = word_counts.iloc[:,:-2]\n",
    "\n",
    "y_pred = KMeans(n_clusters=2, random_state=42).fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1\n",
      " 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1\n",
      " 0 1 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1\n",
      " 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1\n",
      " 0 0 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1\n",
      " 0 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1\n",
      " 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0\n",
      " 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0\n",
      " 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 1 0 0 1\n",
      " 0 0 1 1 1 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 0 1 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1\n",
      " 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1\n",
      " 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1\n",
      " 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 1 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster based on authors and are there certain authors using common words or not? bar graph? potentially try pca too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "6    0\n",
      "7    0\n",
      "8    0\n",
      "9    0\n",
      "10   0\n",
      "11   0\n",
      "12   0\n",
      "13   0\n",
      "14   0\n",
      "15   1\n",
      "16   0\n",
      "17   0\n",
      "18   0\n",
      "19   1\n",
      "20   0\n",
      "21   0\n",
      "22   0\n",
      "23   0\n",
      "24   1\n",
      "25   0\n",
      "26   0\n",
      "27   1\n",
      "28   0\n",
      "29   0\n",
      "..  ..\n",
      "929  0\n",
      "930  0\n",
      "931  0\n",
      "932  0\n",
      "933  0\n",
      "934  0\n",
      "935  0\n",
      "936  0\n",
      "937  0\n",
      "938  0\n",
      "939  0\n",
      "940  0\n",
      "941  0\n",
      "942  0\n",
      "943  0\n",
      "944  0\n",
      "945  0\n",
      "946  0\n",
      "947  0\n",
      "948  0\n",
      "949  0\n",
      "950  0\n",
      "951  0\n",
      "952  0\n",
      "953  0\n",
      "954  0\n",
      "955  0\n",
      "956  0\n",
      "957  0\n",
      "958  0\n",
      "\n",
      "[959 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "def doKmeans(X, nclust=2):\n",
    "    model = KMeans(nclust)\n",
    "    model.fit(X)\n",
    "    clust_labels = model.predict(X)\n",
    "    cent = model.cluster_centers_\n",
    "    return (clust_labels, cent)\n",
    "\n",
    "clust_labels, cent = doKmeans(word_counts.iloc[:,:-2], 2)\n",
    "kmeans = pd.DataFrame(clust_labels)\n",
    "word_counts.insert((word_counts.shape[1]),'kmeans',kmeans)\n",
    "print(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing k-means and mini batch k-means solutions:\n",
      "col_0    0    1\n",
      "row_0          \n",
      "0        5    0\n",
      "1      353  601\n"
     ]
    }
   ],
   "source": [
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=2,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(X)\n",
    "\n",
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing k-means and mini batch k-means solutions:')\n",
    "print(pd.crosstab(predict_mini, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9878260869565217\n",
      "\n",
      "Test set score: 0.46875\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(575, 3049) (575,)\n",
      "Training set score: 0.9704347826086956\n",
      "\n",
      "Test set score: 0.6015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') \n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
