{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amyscott/anaconda2/bin/python: No module named spacy\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amyscott/anaconda2/envs/nthinkfulpython/bin/python\n",
      "3.7.2 (default, Dec 29 2018, 00:00:04) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "sys.version_info(major=3, minor=7, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.15 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "Raw:\n",
      " [Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# Import the data we just downloaded and installed.\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "# Grab and process the raw data.\n",
    "print(gutenberg.fileids())\n",
    "\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# Print the first 100 characters of Alice in Wonderland.\n",
    "print('\\nRaw:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title removed:\n",
      " \n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on\n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "persuasion = re.sub(pattern, \"\", persuasion)\n",
    "alice = re.sub(pattern, \"\", alice)\n",
    "\n",
    "# Print the first 100 characters of Alice again.\n",
    "print('Title removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter headings removed:\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothin\n"
     ]
    }
   ],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "\n",
    "# Ok, what's it look like now?\n",
    "print('Chapter headings removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to\n"
     ]
    }
   ],
   "source": [
    "persuasion = ' '.join(persuasion.split())\n",
    "alice = ' '.join(alice.split())\n",
    "\n",
    "# All done with cleanup? Let's see how it looks.\n",
    "print('Extra whitespace removed:\\n', alice[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The alice_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 34430 tokens long\n",
      "The first three tokens are 'Alice was beginning'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(\"The alice_doc object is a {} object.\".format(type(alice_doc)))\n",
    "print(\"It is {} tokens long\".format(len(alice_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(alice_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(alice_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('the', 1524), ('and', 796), ('to', 724), ('a', 611), ('I', 534), ('it', 524), ('she', 508), ('of', 499), ('said', 453), ('Alice', 394)]\n",
      "Persuasion: [('the', 3120), ('to', 2775), ('and', 2738), ('of', 2563), ('a', 1529), ('in', 1346), ('was', 1329), ('had', 1177), ('her', 1159), ('I', 1121)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "    \n",
    "# The most frequent words:\n",
    "alice_freq = word_frequencies(alice_doc).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: [('I', 534), ('said', 453), ('Alice', 394), (\"n't\", 215), (\"'s\", 190), ('little', 124), ('The', 102), ('like', 84), ('went', 83), ('know', 83)]\n",
      "Persuasion: [('I', 1121), ('Anne', 497), (\"'s\", 485), ('She', 326), ('Captain', 297), ('Mrs', 291), ('Elliot', 288), ('Mr', 255), ('He', 225), ('Wentworth', 217)]\n"
     ]
    }
   ],
   "source": [
    "alice_freq = word_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_freq = word_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('Alice:', alice_freq)\n",
    "print('Persuasion:', persuasion_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Alice: {'Alice', 'went', 'said', \"n't\", 'little', 'like', 'know', 'The'}\n",
      "Unique to Persuasion: {'Anne', 'Mrs', 'He', 'She', 'Mr', 'Elliot', 'Captain', 'Wentworth'}\n"
     ]
    }
   ],
   "source": [
    "alice_common = [pair[0] for pair in alice_freq]\n",
    "persuasion_common = [pair[0] for pair in persuasion_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top ten.\n",
    "print('Unique to Alice:', set(alice_common) - set(persuasion_common))\n",
    "print('Unique to Persuasion:', set(persuasion_common) - set(alice_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: [('-PRON-', 758), ('say', 476), ('alice', 396), ('be', 254), ('not', 231), ('go', 133), ('think', 131), ('little', 126), ('the', 109), ('look', 105)]\n",
      "Persuasion: [('-PRON-', 2241), ('anne', 497), (\"'s\", 466), ('captain', 303), ('elliot', 295), ('mrs', 291), ('good', 289), ('know', 258), ('think', 256), ('mr', 255)]\n",
      "Unique to Alice: {'alice', 'the', 'be', 'little', 'not', 'go', 'say', 'look'}\n",
      "Unique to Persuasion: {'good', \"'s\", 'anne', 'captain', 'mrs', 'know', 'mr', 'elliot'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "alice_lemma_freq = lemma_frequencies(alice_doc, include_stop=False).most_common(10)\n",
    "persuasion_lemma_freq = lemma_frequencies(persuasion_doc, include_stop=False).most_common(10)\n",
    "print('\\nAlice:', alice_lemma_freq)\n",
    "print('Persuasion:', persuasion_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "alice_lemma_common = [pair[0] for pair in alice_lemma_freq]\n",
    "persuasion_lemma_common = [pair[0] for pair in persuasion_lemma_freq]\n",
    "print('Unique to Alice:', set(alice_lemma_common) - set(persuasion_lemma_common))\n",
    "print('Unique to Persuasion:', set(persuasion_lemma_common) - set(alice_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice in Wonderland has 1678 sentences.\n",
      "Here is an example: \n",
      "There was nothing so VERY remarkable in that; nor did Alice think it so VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list(alice_doc.sents)\n",
    "print(\"Alice in Wonderland has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 words in this sentence, and 25 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(nlp(\"I need a break\")[3].pos_)\n",
    "print(nlp(\"I need to break the glass\")[3].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "There ADV\n",
      "was VERB\n",
      "nothing NOUN\n",
      "so ADV\n",
      "VERY ADV\n",
      "remarkable ADJ\n",
      "in ADP\n",
      "that DET\n",
      "; PUNCT\n"
     ]
    }
   ],
   "source": [
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-23-a7fd74f4dfc0>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-a7fd74f4dfc0>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print(token.orth_, token.dep_, token.head.orth_\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Alice\n",
      "DATE the hot day\n",
      "PERSON Alice\n",
      "PRODUCT Rabbit\n",
      "PRODUCT Rabbit\n",
      "PRODUCT WAISTCOAT - POCKET\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "PERSON Alice\n",
      "ORDINAL First\n"
     ]
    }
   ],
   "source": [
    "entities = list(alice_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Drink', 'Stand', 'Beautiful Soup', 'Soup', 'Gryphon', 'Rabbit', 'Sha', 'Curiouser', 'Fifteenth', '--or', 'the Lobster Quadrille?', 'WILLIAM', 'Said', 'Run', 'a Lobster Quadrille', 'this:--', 'Mabel', 'Fish-Footman', 'Queen', 'Shy', 'Lacie', 'Tillie', 'Soles', 'Adventures', 'William', 'Fetch', 'The White Rabbit', 'Serpent', 'Soup of the evening', 'indeed:--', 'Prizes', 'Longitude', 'The Fish-Footman', 'the Lobster Quadrille', 'Bill', 'Pat', 'Latitude', 'Canary', 'Mary Ann', 'Sentence', 'Beau', 'The Mock Turtle', 'Morcar', 'Shall', \"the Mock Turtle: '\", 'Seaography', 'Turn', 'Elsie', 'Idiot', 'the White Rabbit', 'Alice', 'the Mock Turtle', 'Panther', 'Edwin', 'INSIDE', 'Jack', 'Treacle', 'Crab', 'Begin', 'Soo', 'Footman', 'The Queen', 'FUL SOUP', 'Hush', 'Duchess', 'Duck', 'Cheshire Puss', 'Turtle Soup', 'm--', 'Repeat', 'Brandy', 'Kings', \"Don't\", 'Ou', 'Sixteenth', 'William the Conqueror', 'Tut', 'Hjckrrh', 'M--', \"Dinah'll\", 'Pinch', 'YOURS', 'Latin Grammar', 'the King', 'Shakespeare', 'Fury', 'began:--', 'Ma', 'the Duchess', 'the Queen of Hearts', 'Majesty', 'the March Hare', 'HAD', 'Mock Turtle', \"the Duchess: '\", 'Stolen', 'Edgar Atheling', \"the King: '\", 'Down', 'Frog-Footman', 'Stupid'}\n"
     ]
    }
   ],
   "source": [
    "people = [entity.text for entity in list(alice_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amyscott/anaconda2/bin/python: No module named spacy\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice[:int(len(alice)/10)])\n",
    "persuasion = text_cleaner(persuasion[:int(len(persuasion)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assure</th>\n",
       "      <th>rat</th>\n",
       "      <th>beautifier</th>\n",
       "      <th>slightingly</th>\n",
       "      <th>cuff</th>\n",
       "      <th>get</th>\n",
       "      <th>cheat</th>\n",
       "      <th>fact</th>\n",
       "      <th>brave</th>\n",
       "      <th>mahogany</th>\n",
       "      <th>...</th>\n",
       "      <th>talk</th>\n",
       "      <th>likely</th>\n",
       "      <th>manner</th>\n",
       "      <th>burn</th>\n",
       "      <th>venture</th>\n",
       "      <th>violently</th>\n",
       "      <th>form</th>\n",
       "      <th>ship</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1649 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  assure rat beautifier slightingly cuff get cheat fact brave mahogany  ...  \\\n",
       "0      0   0          0           0    0   0     0    0     0        0  ...   \n",
       "1      0   0          0           0    0   1     0    0     0        0  ...   \n",
       "2      0   0          0           0    0   0     0    0     0        0  ...   \n",
       "3      0   0          0           0    0   0     0    0     0        0  ...   \n",
       "4      0   0          0           0    0   0     0    0     0        0  ...   \n",
       "\n",
       "  talk likely manner burn venture violently form ship  \\\n",
       "0    0      0      0    0       0         0    0    0   \n",
       "1    0      0      0    0       0         0    0    0   \n",
       "2    0      0      0    0       0         0    0    0   \n",
       "3    0      0      0    0       0         0    0    0   \n",
       "4    0      0      0    0       0         0    0    0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 1649 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.992\n",
      "\n",
      "Test set score: 0.8203592814371258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amyscott/anaconda2/envs/nthinkfulpython/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 1647) (250,)\n",
      "Training set score: 0.976\n",
      "\n",
      "Test set score: 0.8862275449101796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amyscott/anaconda2/envs/nthinkfulpython/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2') # No need to specify l2 as it's the default. But we put it for demonstration.\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.968\n",
      "\n",
      "Test set score: 0.8083832335329342\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8562874251497006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma[:int(len(emma)/60)])\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.6330275229357798\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>120</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen      120       20\n",
       "Carroll      60       18"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge 0: try maybe random forest, svm? add more characteristics...\n",
    "#challenge 1: doesn't need to be the same authors for new book. is it good to determine austen vs another book? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The family of Dashwood had long been settled in Sussex. Their estate was large, and their residence \n"
     ]
    }
   ],
   "source": [
    "sense = gutenberg.raw('austen-sense.txt')\n",
    "sense = re.sub(r'VOLUME \\w+', '', sense)\n",
    "sense = re.sub(r'CHAPTER \\w+', '', sense)\n",
    "sense = text_cleaner(sense[:int(len(sense)/40)])\n",
    "print(sense[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_doc = nlp(sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "sense_sents = [[sent, \"Austen\"] for sent in sense_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "sense_sentences = pd.DataFrame(sense_sents)\n",
    "sense_bow = bow_features(sense_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 313 is out of bounds for axis 0 with size 250",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-432eb99bfa82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m X_Sense_test = np.concatenate((\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Austen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msense_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_sentence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'text_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ), axis=0)\n\u001b[1;32m      5\u001b[0m y_Sense_test = pd.concat([y_train[y_train=='Austen'],\n",
      "\u001b[0;31mIndexError\u001b[0m: index 313 is out of bounds for axis 0 with size 250"
     ]
    }
   ],
   "source": [
    "X_Sense_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Austen'].index],\n",
    "    sense_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Sense_test = pd.concat([y_train[y_train=='Austen'],\n",
    "                         pd.Series(['Austen'] * sense_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Sense_test, y_Sense_test))\n",
    "lr_Sense_predicted = lr.predict(X_Sense_test)\n",
    "pd.crosstab(y_Sense_test, lr_Sense_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
