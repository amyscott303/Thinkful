{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import matplotlib\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/bin/python\n",
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday= gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "poem = re.sub(pattern, \"\", poem)\n",
    "thursday = re.sub(pattern, \"\", thursday)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "\n",
    "poem = ' '.join(poem.split())\n",
    "thursday = ' '.join(thursday.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping dow\n",
      "Extra whitespace removed:\n",
      " To Edmund Clerihew Bentley A cloud was on the mind of men, and wailing went the weather, Yea, a sick\n"
     ]
    }
   ],
   "source": [
    "print('Extra whitespace removed:\\n', poem[0:100])\n",
    "print('Extra whitespace removed:\\n', thursday[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.max_length\n",
    "\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "\n",
    "common_words = set(poemwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('the', 351), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), ('to', 92), ('my', 72), ('The', 61)]\n",
      "Thursday: [('the', 3290), ('a', 1712), ('of', 1710), ('and', 1568), ('to', 1044), ('in', 887), ('I', 880), ('he', 858), ('that', 840), ('his', 765)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "  \n",
    "    return Counter(words)\n",
    "    \n",
    "poem_freq = word_frequencies(poem_doc).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc).most_common(10)\n",
    "print('Poem:', poem_freq)\n",
    "print('Thursday:', thursday_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('And', 176), ('I', 130), ('The', 61), (\"'s\", 43), ('thee', 42), ('like', 29), ('thy', 28), ('thou', 28), ('THE', 27), ('little', 26)]\n",
      "Thursday: [('I', 880), ('Syme', 512), ('said', 495), ('The', 325), ('man', 272), ('He', 268), ('like', 260), (\"'s\", 223), ('But', 161), ('It', 152)]\n"
     ]
    }
   ],
   "source": [
    "poem_freq = word_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "print('Poem:', poem_freq)\n",
    "print('Thursday:', thursday_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Poem: {'And', 'thee', 'thy', 'thou', 'THE', 'little'}\n",
      "Unique to Thursday: {'He', 'said', 'Syme', 'It', 'man', 'But'}\n"
     ]
    }
   ],
   "source": [
    "poem_common = [pair[0] for pair in poem_freq]\n",
    "thursday_common = [pair[0] for pair in thursday_freq]\n",
    "\n",
    "print('Unique to Poem:', set(poem_common) - set(thursday_common))\n",
    "print('Unique to Thursday:', set(thursday_common) - set(poem_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('-PRON-', 204), ('and', 179), ('the', 88), (\"'s\", 45), ('little', 45), ('thee', 42), ('weep', 35), ('like', 35), ('thou', 35), ('hear', 33)]\n",
      "Thursday: [('-PRON-', 1712), ('syme', 516), ('say', 510), ('man', 365), ('the', 344), ('like', 268), (\"'s\", 164), ('look', 163), ('but', 161), ('come', 161)]\n",
      "Unique to Poem: {'thee', 'and', 'thou', 'hear', 'weep', 'little'}\n",
      "Unique to Thursday: {'come', 'say', 'syme', 'but', 'man', 'look'}\n"
     ]
    }
   ],
   "source": [
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    return Counter(lemmas)\n",
    "\n",
    "\n",
    "poem_lemma_freq = lemma_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_lemma_freq = lemma_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "print('Poem:', poem_lemma_freq)\n",
    "print('Thursday:', thursday_lemma_freq)\n",
    "\n",
    "poem_lemma_common = [pair[0] for pair in poem_lemma_freq]\n",
    "thursday_lemma_common = [pair[0] for pair in thursday_lemma_freq]\n",
    "print('Unique to Poem:', set(poem_lemma_common) - set(thursday_lemma_common))\n",
    "print('Unique to Thursday:', set(thursday_lemma_common) - set(poem_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem has 498 sentences.\n",
      "Here is an example: \n",
      "So I piped with merry cheer. \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list(poem_doc.sents)\n",
    "print(\"Poem has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 words in this sentence, and 6 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(nlp(\"I need a break\")[3].pos_)\n",
    "print(nlp(\"I need to break the glass\")[3].pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "So ADV\n",
      "I PRON\n",
      "piped VERB\n",
      "with ADP\n",
      "merry NOUN\n",
      "cheer NOUN\n",
      ". PUNCT\n",
      "\" PUNCT\n"
     ]
    }
   ],
   "source": [
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "So advmod piped\n",
      "I nsubj piped\n",
      "piped ROOT piped\n",
      "with prep piped\n",
      "merry compound cheer\n",
      "cheer pobj with\n",
      ". punct piped\n",
      "\" punct piped\n"
     ]
    }
   ],
   "source": [
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Edmund Clerihew Bentley\n",
      "PERSON Yea\n",
      "DATE Lust\n",
      "ORG Whistler\n",
      "PERSON Baal\n",
      "TIME the hour\n",
      "CARDINAL ten million\n",
      "ORG Truth\n",
      "ORG Tusitala\n",
      "PERSON Dunedin\n"
     ]
    }
   ],
   "source": [
    "entities = list(thursday_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Syme', 'Superstition', 'Joseph Chamberlain', 'Law', 'Baal', 'Rosamond', 'Edmund Clerihew Bentley', 'Bradshaw', 'Queen Anne', 'Victoria', 'Gabriel Syme', 'Gregory', 'Dunedin', 'Adam', 'Colney Hatch', 'Miss Gregory', 'Chamberlain', 'Byron', 'Wrongs', 'G. K. C.', 'Wrong', 'Baker', 'Rosamond Gregory', 'Lucian Gregory', 'Yea'}\n"
     ]
    }
   ],
   "source": [
    "people = [entity.text for entity in list(thursday_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "    \n",
    "thursday = text_cleaner(thursday[:int(len(thursday)/10)])\n",
    "poem = text_cleaner(poem[:int(len(poem)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(INTRODUCTION, Piping, down, the, valleys, wil...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(So, I, piped, with, merry, cheer, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Piper, ,, pipe, that, song, again)</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(;, \", So, I, piped, :, he, wept, to, hear, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1\n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...  Poem\n",
       "1  (INTRODUCTION, Piping, down, the, valleys, wil...  Poem\n",
       "2           (So, I, piped, with, merry, cheer, ., \")  Poem\n",
       "3                (Piper, ,, pipe, that, song, again)  Poem\n",
       "4  (;, \", So, I, piped, :, he, wept, to, hear, ., \")  Poem"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_sents = [[sent, \"Poem\"] for sent in poem_doc.sents]\n",
    "thursday_sents = [[sent, \"Thursday\"] for sent in thursday_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(poem_sents + thursday_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(poemwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revolutionise</th>\n",
       "      <th>artistic</th>\n",
       "      <th>screw</th>\n",
       "      <th>formality</th>\n",
       "      <th>network</th>\n",
       "      <th>shade</th>\n",
       "      <th>ewe</th>\n",
       "      <th>answer</th>\n",
       "      <th>rebellious</th>\n",
       "      <th>summon</th>\n",
       "      <th>...</th>\n",
       "      <th>only</th>\n",
       "      <th>gross</th>\n",
       "      <th>strength</th>\n",
       "      <th>nightmare</th>\n",
       "      <th>unpretentious</th>\n",
       "      <th>rejoice</th>\n",
       "      <th>kindly</th>\n",
       "      <th>bulky</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(INTRODUCTION, Piping, down, the, valleys, wil...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, I, piped, with, merry, cheer, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Piper, ,, pipe, that, song, again)</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(;, \", So, I, piped, :, he, wept, to, hear, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  revolutionise artistic screw formality network shade ewe answer rebellious  \\\n",
       "0             0        0     0         0       0     0   0      0          0   \n",
       "1             0        0     0         0       0     0   0      0          0   \n",
       "2             0        0     0         0       0     0   0      0          0   \n",
       "3             0        0     0         0       0     0   0      0          0   \n",
       "4             0        0     0         0       0     0   0      0          0   \n",
       "\n",
       "  summon  ... only gross strength nightmare unpretentious rejoice kindly  \\\n",
       "0      0  ...    0     0        0         0             0       0      0   \n",
       "1      0  ...    0     0        0         0             0       0      0   \n",
       "2      0  ...    0     0        0         0             0       0      0   \n",
       "3      0  ...    0     0        0         0             0       0      0   \n",
       "4      0  ...    0     0        0         0             0       0      0   \n",
       "\n",
       "  bulky                                      text_sentence text_source  \n",
       "0     0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...        Poem  \n",
       "1     0  (INTRODUCTION, Piping, down, the, valleys, wil...        Poem  \n",
       "2     0           (So, I, piped, with, merry, cheer, ., \")        Poem  \n",
       "3     0                (Piper, ,, pipe, that, song, again)        Poem  \n",
       "4     0  (;, \", So, I, piped, :, he, wept, to, hear, ., \")        Poem  \n",
       "\n",
       "[5 rows x 1357 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9960474308300395\n",
      "\n",
      "Test set score: 0.9289940828402367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 1355) (253,)\n",
      "Training set score: 0.9802371541501976\n",
      "\n",
      "Test set score: 0.9349112426035503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The Man Who Was Thursday by G . K . Chesterton 1908 ]', 'To Edmund Clerihew Bentley', 'A cloud was on the mind of men , and wailing went the weather , Yea , a sick cloud upon the soul when we were boys together .', 'Not all unhelped we held the fort , our tiny flags unfurled ; Some giants laboured in that cloud to lift it from the world .']\n"
     ]
    }
   ],
   "source": [
    "thursday=gutenberg.paras('chesterton-thursday.txt')\n",
    "#processing\n",
    "thursday_paras=[]\n",
    "for paragraph in thursday:\n",
    "    para=paragraph[0]\n",
    "    \n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    \n",
    "    thursday_paras.append(' '.join(para))\n",
    "\n",
    "print(thursday_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1114\n",
      "Original sentence: \" I tell you it can ' t be !\"\n",
      "Tf_idf vector: {'tell': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(thursday_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=2, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "thursday_paras_tfidf=vectorizer.fit_transform(thursday_paras)\n",
    "print(\"Number of features: %d\" % thursday_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(thursday_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 52.387113674657584\n",
      "Component 0:\n",
      "\" But it was a lovely catechism ,\" said Syme pathetically .                                0.872677\n",
      "\" Not I ,\" said Syme .                                                                     0.872677\n",
      "\" No ,\" said Syme .                                                                        0.872677\n",
      "\" No ,\" said Syme with equal decision .                                                    0.844833\n",
      "\" Thank you ,\" said Syme , \" you flatter me .\"                                             0.844574\n",
      "\" Ah , it was what he said !\"                                                              0.828858\n",
      "\" I have done it ,\" he said hoarsely .                                                     0.828858\n",
      "\" You are willing , that is enough ,\" said the unknown .                                   0.828858\n",
      "\" In what I said ?\"                                                                        0.828858\n",
      "\" There again ,\" said Syme irritably , \" what is there poetical about being in revolt ?    0.793488\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "CHAPTER X       1.0\n",
      "CHAPTER VI      1.0\n",
      "CHAPTER IX      1.0\n",
      "CHAPTER I       1.0\n",
      "CHAPTER XI      1.0\n",
      "CHAPTER V       1.0\n",
      "CHAPTER IV      1.0\n",
      "CHAPTER XIV     1.0\n",
      "CHAPTER XIII    1.0\n",
      "CHAPTER II      1.0\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Get on ,\" said Dr . Bull .                                                                   0.498939\n",
      "\" No ,\" said Dr . Bull in adamantine humility , \" it is I .\"                                   0.498939\n",
      "\" But we may not get it ,\" said Bull .                                                         0.458790\n",
      "Bull said                                                                                      0.458790\n",
      "\" I protest that this is most irregular ,\" said Dr . Bull indignantly .                        0.441510\n",
      "\" Let us have some drinks ,\" said Dr . Bull , after a silence .                                0.411411\n",
      "\" Not so bad as that ,\" said Dr . Bull , with unnecessary laughter , \" not so bad as that .    0.411342\n",
      "\" We might have fought easily ,\" said Bull ; \" we were four against three .\"                   0.404188\n",
      "\" No ,\" said Dr . Bull , \" I hope it won ' t .                                                 0.374963\n",
      "\" I think ,\" said Dr . Bull with precision , \" that I am lying in bed at No .                  0.366062\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "THE PROFESSOR EXPLAINS                                                                     0.875772\n",
      "\" Wake up , Professor !\"                                                                   0.838727\n",
      "The Professor still feared that all was lost ; but he was loyal .                          0.830036\n",
      "THE UNACCOUNTABLE CONDUCT OF PROFESSOR DE WORMS                                            0.740177\n",
      "\" Clashing his hoofs ,\" said the Professor .                                               0.702318\n",
      "The Professor gave a tired gesture .                                                       0.670800\n",
      "The Professor seized Syme roughly by the waistcoat .                                       0.647425\n",
      "\" I ' m in the same boat ,\" said the Professor .                                           0.626945\n",
      "The Professor was continuing his speech , but in the middle of it Syme decided to act .    0.615257\n",
      "\" I have denounced him ,\" answered the Professor .                                         0.609956\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Dr . Bull tossed his sword into the sea .                                                                      0.592944\n",
      "\" Dr .                                                                                                         0.589876\n",
      "Dr . Bull was bubbling over with laughter , swinging the sword in his hand as carelessly as a cane .           0.564968\n",
      "\" Get on ,\" said Dr . Bull .                                                                                   0.532618\n",
      "\" No ,\" said Dr . Bull in adamantine humility , \" it is I .\"                                                   0.532618\n",
      "\" Not so bad as that ,\" said Dr . Bull , with unnecessary laughter , \" not so bad as that .                    0.496096\n",
      "\" Well , because he ' s so jolly like a balloon himself ,\" said Dr . Bull desperately .                        0.490904\n",
      "\" Dr . Bull ,\" said Syme , in a voice peculiarly precise and courteous , \" would you do me a small favour ?    0.482117\n",
      "Dr . Bull smiled again , but continued to gaze on them without speaking .                                      0.466788\n",
      "\" I protest that this is most irregular ,\" said Dr . Bull indignantly .                                        0.460644\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 \" I could not have conceived it ,\" said Syme gravely .\n",
      "1 \" That ' s a pity ,\" he said , \" because I am .\"\n",
      "2 \" Yes ,\" said Syme very seriously , \" a promise .\n",
      "3 \" What is that noise ?\"\n",
      "4 They took their hats and sticks in silence ; but as Syme took his sword - stick , he held it hard .\n",
      "5 \" I tell you it can ' t be !\"\n",
      "6 \" Or yet again ,\" tapped Syme , \" it is positive , as is the passionate red hair of a beautiful woman .\"\n",
      "7 \" Seeking a quarrel with me !\"\n",
      "8 He gave two long whistles , and a hansom came rattling down the road .\n",
      "9 From the doorway there came a murmur of \" Mr . Joseph Chamberlain .\"\n"
     ]
    }
   ],
   "source": [
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
