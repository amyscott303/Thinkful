{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amyscott/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /anaconda3/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "    /anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import matplotlib\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/anaconda3/bin/python\n",
      "3.7.3 (default, Mar 27 2019, 16:54:48) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "sys.version_info(major=3, minor=7, micro=3, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, stopwords\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the gutenberg library I chose to use the Blake- poem and Thursday by Chesterton. I thought this would be a fun and unique comparison since the type of language style and text you use when writing a poem is very different from when writing a novel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data begins now. First I will get rid of punctuation and the double dash --, to allow for it to be easier for it to count and vectorize the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday= gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "poem = re.sub(pattern, \"\", poem)\n",
    "thursday = re.sub(pattern, \"\", thursday)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to remove the chapter titles and the newlines by splitting and rejoining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "\n",
    "poem = ' '.join(poem.split())\n",
    "thursday = ' '.join(thursday.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra whitespace removed:\n",
      " SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping dow\n",
      "Extra whitespace removed:\n",
      " To Edmund Clerihew Bentley A cloud was on the mind of men, and wailing went the weather, Yea, a sick\n"
     ]
    }
   ],
   "source": [
    "print('Extra whitespace removed:\\n', poem[0:100])\n",
    "print('Extra whitespace removed:\\n', thursday[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "nlp.max_length\n",
    "\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "\n",
    "common_words = set(poemwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('the', 351), ('And', 176), ('and', 169), ('of', 131), ('I', 130), ('in', 116), ('a', 108), ('to', 92), ('my', 72), ('The', 61)]\n",
      "Thursday: [('the', 3290), ('a', 1712), ('of', 1710), ('and', 1568), ('to', 1044), ('in', 887), ('I', 880), ('he', 858), ('that', 840), ('his', 765)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "  \n",
    "    return Counter(words)\n",
    "    \n",
    "poem_freq = word_frequencies(poem_doc).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc).most_common(10)\n",
    "print('Poem:', poem_freq)\n",
    "print('Thursday:', thursday_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing what the most frequently used words are in the text, however most of these are all the stop words so now will add a condition to drop the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('And', 176), ('I', 130), ('The', 61), (\"'s\", 43), ('thee', 42), ('like', 29), ('thy', 28), ('thou', 28), ('THE', 27), ('little', 26)]\n",
      "Thursday: [('I', 880), ('Syme', 512), ('said', 495), ('The', 325), ('man', 272), ('He', 268), ('like', 260), (\"'s\", 223), ('But', 161), ('It', 152)]\n"
     ]
    }
   ],
   "source": [
    "poem_freq = word_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_freq = word_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "print('Poem:', poem_freq)\n",
    "print('Thursday:', thursday_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to determine which are unique to the different text, by subracting the common words from the sets of text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Poem: {'THE', 'thy', 'thou', 'And', 'thee', 'little'}\n",
      "Unique to Thursday: {'But', 'He', 'man', 'Syme', 'said', 'It'}\n"
     ]
    }
   ],
   "source": [
    "poem_common = [pair[0] for pair in poem_freq]\n",
    "thursday_common = [pair[0] for pair in thursday_freq]\n",
    "\n",
    "print('Unique to Poem:', set(poem_common) - set(thursday_common))\n",
    "print('Unique to Thursday:', set(thursday_common) - set(poem_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to count the lemmas or the (root/concept) of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem: [('-PRON-', 204), ('and', 179), ('the', 88), (\"'s\", 45), ('little', 45), ('thee', 42), ('weep', 35), ('like', 35), ('thou', 35), ('hear', 33)]\n",
      "Thursday: [('-PRON-', 1712), ('syme', 516), ('say', 510), ('man', 365), ('the', 344), ('like', 268), (\"'s\", 164), ('look', 163), ('but', 161), ('come', 161)]\n",
      "Unique to Poem: {'thou', 'thee', 'little', 'hear', 'weep', 'and'}\n",
      "Unique to Thursday: {'syme', 'man', 'but', 'say', 'come', 'look'}\n"
     ]
    }
   ],
   "source": [
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    return Counter(lemmas)\n",
    "\n",
    "\n",
    "poem_lemma_freq = lemma_frequencies(poem_doc, include_stop=False).most_common(10)\n",
    "thursday_lemma_freq = lemma_frequencies(thursday_doc, include_stop=False).most_common(10)\n",
    "print('Poem:', poem_lemma_freq)\n",
    "print('Thursday:', thursday_lemma_freq)\n",
    "\n",
    "poem_lemma_common = [pair[0] for pair in poem_lemma_freq]\n",
    "thursday_lemma_common = [pair[0] for pair in thursday_lemma_freq]\n",
    "print('Unique to Poem:', set(poem_lemma_common) - set(thursday_lemma_common))\n",
    "print('Unique to Thursday:', set(thursday_lemma_common) - set(poem_lemma_common))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further clarify the context and count of the text I will take a look at the sentences, the words used in the sentences as well as the parts of speech. I want to clarify that certain words like \"break\" don't always have to mean the same thing, it is conditional on how it used in the sentence and the parts of speech surrounding it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem has 498 sentences.\n",
      "Here is an example: \n",
      "So I piped with merry cheer. \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = list(poem_doc.sents)\n",
    "print(\"Poem has {} sentences.\".format(len(sentences)))\n",
    "\n",
    "example_sentence = sentences[2]\n",
    "print(\"Here is an example: \\n{}\\n\".format(example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 words in this sentence, and 6 of them are unique.\n"
     ]
    }
   ],
   "source": [
    "example_words = [token for token in example_sentence if not token.is_punct]\n",
    "unique_words = set([token.text for token in example_words])\n",
    "\n",
    "print((\"There are {} words in this sentence, and {} of them are\"\n",
    "       \" unique.\").format(len(example_words), len(unique_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(nlp(\"I need a break\")[3].pos_)\n",
    "print(nlp(\"I need to break the glass\")[3].pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of speech:\n",
      "So ADV\n",
      "I PRON\n",
      "piped VERB\n",
      "with ADP\n",
      "merry NOUN\n",
      "cheer NOUN\n",
      ". PUNCT\n",
      "\" PUNCT\n"
     ]
    }
   ],
   "source": [
    "print('\\nParts of speech:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependencies:\n",
      "So advmod piped\n",
      "I nsubj piped\n",
      "piped ROOT piped\n",
      "with prep piped\n",
      "merry compound cheer\n",
      "cheer pobj with\n",
      ". punct piped\n",
      "\" punct piped\n"
     ]
    }
   ],
   "source": [
    "print('\\nDependencies:')\n",
    "for token in example_sentence[:9]:\n",
    "    print(token.orth_, token.dep_, token.head.orth_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON Edmund Clerihew Bentley\n",
      "PERSON Yea\n",
      "PERSON Whistler\n",
      "PERSON Baal\n",
      "TIME the hour\n",
      "CARDINAL ten million\n",
      "ORG Tusitala\n",
      "PERSON Dunedin\n",
      "GPE Samoa\n",
      "GPE the City of Mansoul\n"
     ]
    }
   ],
   "source": [
    "entities = list(thursday_doc.ents)[0:10]\n",
    "for entity in entities:\n",
    "    print(entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'de Worms', 'Battersea', 'Burgundy', \"Ally Sloper's\", 'Bibles', 'Surrey', 'Colney Hatch', 'Gabriel Syme', 'Superstition', 'Buttons', 'Bannockburn', 'Rosamond Gregory', 'Witherspoon', 'Ratcliffe', 'Shall', 'Baal', 'Bull--', 'Strike', 'Gogol', 'Sussex', 'Queen Victoria', 'Cold', 'Ducroix', 'Kill', 'Law', 'Lucian Gregory', 'Norman', 'Alhambra', 'Rosamond', 'Clapham Common', 'Lush', 'Chamberlain', 'Wrong', 'G. K. C.', 'Judas', 'Chretiens', 'Renard', 'Caesar', 'Dunedin', 'Harrow', 'Aesop', 'Yea', 'Joseph Chamberlain', 'Mohammedan', 'Zumpt', 'Virtue', 'Joseph Chamberlains', 'Erie', 'Queen Anne', 'Captain Bellegarde', 'Jacobins', 'Bradshaw', 'Dawn', 'Thames', 'Scotch', 'Robespierre', 'Poe', 'De Worms', 'Baker', 'Miss Gregory', 'Inspector Ratcliffe', 'Rembrandt', 'Gabriel', 'Pan', 'God', 'Jabberwock', 'Jack-in', 'DE WORMS', 'if--', 'Colt', 'Wilks', 'Victoria', 'Gregory', 'Zso', 'Attila', 'Biffin', 'Chinaman', 'Better', 'Alice', 'Horatius', 'Adam', 'EXPLAINS', 'Gemini', 'FRIEND', 'Gendarmerie', 'Buck', 'George', \"lush'--word\", 'Edmund Clerihew Bentley', 'Martin Tupper', 'Wrongs', 'Nature', 'Dickens', 'GABRIEL SYME', 'Syme', 'Absurd', 'Marquis', 'Roman', 'Burnt', 'Place de', 'Bull', 'Whistler', 'Bulwer Lytton', 'Tim Healy', 'Twice Syme', 'Lo', 'Byron', 'Extraordinary', 'Bruce', 'Pole', 'DUEL SYME'}\n"
     ]
    }
   ],
   "source": [
    "people = [entity.text for entity in list(thursday_doc.ents) if entity.label_ == \"PERSON\"]\n",
    "print(set(people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "poem = gutenberg.raw('blake-poems.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "poem = re.sub(r'Chapter \\d+', '', poem)\n",
    "thursday = re.sub(r'CHAPTER .*', '', thursday)\n",
    "    \n",
    "thursday = text_cleaner(thursday[:int(len(thursday)/10)])\n",
    "poem = text_cleaner(poem[:int(len(poem)/10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "poem_doc = nlp(poem)\n",
    "thursday_doc = nlp(thursday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(INTRODUCTION, Piping, down, the, valleys, wil...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(So, I, piped, with, merry, cheer, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Piper, ,, pipe, that, song, again)</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(;, \", So, I, piped, :, he, wept, to, hear, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0     1\n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...  Poem\n",
       "1  (INTRODUCTION, Piping, down, the, valleys, wil...  Poem\n",
       "2           (So, I, piped, with, merry, cheer, ., \")  Poem\n",
       "3                (Piper, ,, pipe, that, song, again)  Poem\n",
       "4  (;, \", So, I, piped, :, he, wept, to, hear, ., \")  Poem"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_sents = [[sent, \"Poem\"] for sent in poem_doc.sents]\n",
    "thursday_sents = [[sent, \"Thursday\"] for sent in thursday_doc.sents]\n",
    "\n",
    "# Combining the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(poem_sents + thursday_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    \n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "       \n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "poemwords = bag_of_words(poem_doc)\n",
    "thursdaywords = bag_of_words(thursday_doc)\n",
    "\n",
    "\n",
    "common_words = set(poemwords + thursdaywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 50\n",
      "Processing row 100\n",
      "Processing row 150\n",
      "Processing row 200\n",
      "Processing row 250\n",
      "Processing row 300\n",
      "Processing row 350\n",
      "Processing row 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reign</th>\n",
       "      <th>abolish</th>\n",
       "      <th>discover</th>\n",
       "      <th>grove</th>\n",
       "      <th>cliff</th>\n",
       "      <th>duel</th>\n",
       "      <th>bare</th>\n",
       "      <th>thread</th>\n",
       "      <th>place</th>\n",
       "      <th>shadow</th>\n",
       "      <th>...</th>\n",
       "      <th>unnatural</th>\n",
       "      <th>water</th>\n",
       "      <th>cosy</th>\n",
       "      <th>second</th>\n",
       "      <th>comedy</th>\n",
       "      <th>pour</th>\n",
       "      <th>outburst</th>\n",
       "      <th>waistcoat</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(INTRODUCTION, Piping, down, the, valleys, wil...</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, I, piped, with, merry, cheer, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Piper, ,, pipe, that, song, again)</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(;, \", So, I, piped, :, he, wept, to, hear, ., \")</td>\n",
       "      <td>Poem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  reign abolish discover grove cliff duel bare thread place shadow  ...  \\\n",
       "0     0       0        0     0     0    0    0      0     0      0  ...   \n",
       "1     0       0        0     0     0    0    0      0     0      0  ...   \n",
       "2     0       0        0     0     0    0    0      0     0      0  ...   \n",
       "3     0       0        0     0     0    0    0      0     0      0  ...   \n",
       "4     0       0        0     0     0    0    0      0     0      0  ...   \n",
       "\n",
       "  unnatural water cosy second comedy pour outburst waistcoat  \\\n",
       "0         0     0    0      0      0    0        0         0   \n",
       "1         0     0    0      0      0    0        0         0   \n",
       "2         0     0    0      0      0    0        0         0   \n",
       "3         0     0    0      0      0    0        0         0   \n",
       "4         0     0    0      0      0    0        0         0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (SONGS, OF, INNOCENCE, AND, OF, EXPERIENCE, an...        Poem  \n",
       "1  (INTRODUCTION, Piping, down, the, valleys, wil...        Poem  \n",
       "2           (So, I, piped, with, merry, cheer, ., \")        Poem  \n",
       "3                (Piper, ,, pipe, that, song, again)        Poem  \n",
       "4  (;, \", So, I, piped, :, he, wept, to, hear, ., \")        Poem  \n",
       "\n",
       "[5 rows x 1357 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9960474308300395\n",
      "\n",
      "Test set score: 0.9408284023668639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 1355) (253,)\n",
      "Training set score: 0.9802371541501976\n",
      "\n",
      "Test set score: 0.9349112426035503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2')\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The Man Who Was Thursday by G . K . Chesterton 1908 ]', 'To Edmund Clerihew Bentley', 'A cloud was on the mind of men , and wailing went the weather , Yea , a sick cloud upon the soul when we were boys together .', 'Not all unhelped we held the fort , our tiny flags unfurled ; Some giants laboured in that cloud to lift it from the world .']\n"
     ]
    }
   ],
   "source": [
    "thursday=gutenberg.paras('chesterton-thursday.txt')\n",
    "\n",
    "thursday_paras=[]\n",
    "for paragraph in thursday:\n",
    "    para=paragraph[0]\n",
    "    \n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    \n",
    "    thursday_paras.append(' '.join(para))\n",
    "\n",
    "print(thursday_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1114\n",
      "Original sentence: \" I tell you it can ' t be !\"\n",
      "Tf_idf vector: {'tell': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(thursday_paras, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                             min_df=2, \n",
    "                             stop_words='english', \n",
    "                             lowercase=True, \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "thursday_paras_tfidf=vectorizer.fit_transform(thursday_paras)\n",
    "print(\"Number of features: %d\" % thursday_paras_tfidf.get_shape()[1])\n",
    "\n",
    "\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(thursday_paras_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 52.422739202071234\n",
      "Component 0:\n",
      "\" But it was a lovely catechism ,\" said Syme pathetically .                                0.872183\n",
      "\" Not I ,\" said Syme .                                                                     0.872183\n",
      "\" No ,\" said Syme .                                                                        0.872183\n",
      "\" No ,\" said Syme with equal decision .                                                    0.844256\n",
      "\" Thank you ,\" said Syme , \" you flatter me .\"                                             0.843113\n",
      "\" Ah , it was what he said !\"                                                              0.829215\n",
      "\" I have done it ,\" he said hoarsely .                                                     0.829215\n",
      "\" You are willing , that is enough ,\" said the unknown .                                   0.829215\n",
      "\" In what I said ?\"                                                                        0.829215\n",
      "\" There again ,\" said Syme irritably , \" what is there poetical about being in revolt ?    0.792669\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "CHAPTER I       1.0\n",
      "CHAPTER XIII    1.0\n",
      "CHAPTER XI      1.0\n",
      "CHAPTER II      1.0\n",
      "CHAPTER III     1.0\n",
      "CHAPTER IV      1.0\n",
      "CHAPTER V       1.0\n",
      "CHAPTER IX      1.0\n",
      "CHAPTER X       1.0\n",
      "CHAPTER XIV     1.0\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Get on ,\" said Dr . Bull .                                                                   0.499691\n",
      "\" No ,\" said Dr . Bull in adamantine humility , \" it is I .\"                                   0.499691\n",
      "Bull said                                                                                      0.457180\n",
      "\" But we may not get it ,\" said Bull .                                                         0.457180\n",
      "\" I protest that this is most irregular ,\" said Dr . Bull indignantly .                        0.436040\n",
      "\" Not so bad as that ,\" said Dr . Bull , with unnecessary laughter , \" not so bad as that .    0.427696\n",
      "\" Let us have some drinks ,\" said Dr . Bull , after a silence .                                0.409329\n",
      "\" We might have fought easily ,\" said Bull ; \" we were four against three .\"                   0.402475\n",
      "\" No ,\" said Dr . Bull , \" I hope it won ' t .                                                 0.372810\n",
      "\" I think ,\" said Dr . Bull with precision , \" that I am lying in bed at No .                  0.360661\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "THE PROFESSOR EXPLAINS                                                                     0.875739\n",
      "The Professor still feared that all was lost ; but he was loyal .                          0.834110\n",
      "\" Wake up , Professor !\"                                                                   0.832587\n",
      "THE UNACCOUNTABLE CONDUCT OF PROFESSOR DE WORMS                                            0.741484\n",
      "The Professor gave a tired gesture .                                                       0.706188\n",
      "\" Clashing his hoofs ,\" said the Professor .                                               0.702390\n",
      "The Professor seized Syme roughly by the waistcoat .                                       0.646927\n",
      "\" I ' m in the same boat ,\" said the Professor .                                           0.639026\n",
      "The Professor was continuing his speech , but in the middle of it Syme decided to act .    0.624760\n",
      "\" I have denounced him ,\" answered the Professor .                                         0.610441\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Dr . Bull tossed his sword into the sea .                                                                      0.592655\n",
      "\" Dr .                                                                                                         0.592005\n",
      "Dr . Bull was bubbling over with laughter , swinging the sword in his hand as carelessly as a cane .           0.568056\n",
      "\" No ,\" said Dr . Bull in adamantine humility , \" it is I .\"                                                   0.533421\n",
      "\" Get on ,\" said Dr . Bull .                                                                                   0.533421\n",
      "\" Not so bad as that ,\" said Dr . Bull , with unnecessary laughter , \" not so bad as that .                    0.515826\n",
      "\" Well , because he ' s so jolly like a balloon himself ,\" said Dr . Bull desperately .                        0.485554\n",
      "\" Dr . Bull ,\" said Syme , in a voice peculiarly precise and courteous , \" would you do me a small favour ?    0.482396\n",
      "Dr . Bull smiled again , but continued to gaze on them without speaking .                                      0.461564\n",
      "\" I protest that this is most irregular ,\" said Dr . Bull indignantly .                                        0.454939\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 \" I could not have conceived it ,\" said Syme gravely .\n",
      "1 \" That ' s a pity ,\" he said , \" because I am .\"\n",
      "2 \" Yes ,\" said Syme very seriously , \" a promise .\n",
      "3 \" What is that noise ?\"\n",
      "4 They took their hats and sticks in silence ; but as Syme took his sword - stick , he held it hard .\n",
      "5 \" I tell you it can ' t be !\"\n",
      "6 \" Or yet again ,\" tapped Syme , \" it is positive , as is the passionate red hair of a beautiful woman .\"\n",
      "7 \" Seeking a quarrel with me !\"\n",
      "8 He gave two long whistles , and a hansom came rattling down the road .\n",
      "9 From the doorway there came a murmur of \" Mr . Joseph Chamberlain .\"\n"
     ]
    }
   ],
   "source": [
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
